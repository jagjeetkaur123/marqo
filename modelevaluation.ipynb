{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install deepchecks -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70          0.0             1.9      0.076   \n",
       "1            7.8              0.88          0.0             2.6      0.098   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepchecks.tabular.datasets.regression import wine_quality\n",
    "data = wine_quality.load_data(data_format = 'Dataframe', as_train_test = False)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data['quality'], test_size = 0.2, random_state = 42)\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "train_ds = Dataset(X_train, label = y_train, cat_features = [])\n",
    "test_ds = Dataset(X_test, label= y_test, cat_features = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <label>\n",
       "                    Model Evaluation Suite:<br/>\n",
       "                    <progress\n",
       "                        value='12'\n",
       "                        max='12'\n",
       "                        class='deepchecks'\n",
       "                    >\n",
       "                    </progress>\n",
       "                </label>\n",
       "                <span>12/12 [Time: 00:30, Check=Model Inference Time]</span>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab940d4b34149cda34cf589410c312c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n            <h1 id=\"summary_0MZX05A8KJCA3UBFSQ0LGY8KA\">Model …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "evaluation_suite = model_evaluation()\n",
    "suite_result = evaluation_suite.run(train_ds, test_ds, gbr)\n",
    "suite_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af789adf76b45fb818a9ea475729023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Train Test Performance</b></h4>'), HTML(value='<p>Summarize given model perf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.checks import TrainTestPerformance\n",
    "gbr = GradientBoostingRegressor(n_estimators= 20)\n",
    "gbr.fit(X_train, y_train)\n",
    "check = TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.3)\n",
    "result = check.run(train_ds, test_ds, gbr)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Evaluation Suite: [\n",
       "\t0: TrainTestPerformance\n",
       "\t\tConditions:\n",
       "\t\t\t0: Train-Test scores relative degradation is less than 0.1\n",
       "\t1: RocReport(excluded_classes=[])\n",
       "\t\tConditions:\n",
       "\t\t\t0: AUC score for all the classes is greater than 0.7\n",
       "\t2: ConfusionMatrixReport\n",
       "\t3: TrainTestPredictionDrift\n",
       "\t\tConditions:\n",
       "\t\t\t0: categorical drift score < 0.15 and numerical drift score < 0.075\n",
       "\t4: SimpleModelComparison\n",
       "\t\tConditions:\n",
       "\t\t\t0: Model performance gain over simple model is greater than 10%\n",
       "\t5: WeakSegmentsPerformance(n_to_show=5)\n",
       "\t\tConditions:\n",
       "\t\t\t0: The relative performance of weakest segment is greater than 80% of average model performance.\n",
       "\t6: CalibrationScore\n",
       "\t7: RegressionSystematicError\n",
       "\t\tConditions:\n",
       "\t\t\t0: Bias ratio is less than 0.01\n",
       "\t8: RegressionErrorDistribution\n",
       "\t\tConditions:\n",
       "\t\t\t0: Kurtosis value is greater than -0.1\n",
       "\t9: UnusedFeatures\n",
       "\t\tConditions:\n",
       "\t\t\t0: Number of high variance unused features is less or equal to 5\n",
       "\t10: BoostingOverfit\n",
       "\t\tConditions:\n",
       "\t\t\t0: Test score over iterations is less than 5% from the best score\n",
       "\t11: ModelInferenceTime\n",
       "\t\tConditions:\n",
       "\t\t\t0: Average model inference time for one sample is less than 0.001\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_suite[0].clean_conditions()\n",
    "evaluation_suite[0].add_condition_train_test_relative_degradation_less_than(0.3)\n",
    "evaluation_suite = evaluation_suite.remove(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <label>\n",
       "                    Model Evaluation Suite:<br/>\n",
       "                    <progress\n",
       "                        value='8'\n",
       "                        max='8'\n",
       "                        class='deepchecks'\n",
       "                    >\n",
       "                    </progress>\n",
       "                </label>\n",
       "                <span>8/8 [Time: 00:27, Check=Model Inference Time]</span>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluation_suite.run(train_ds, test_ds, gbr)\n",
    "result.passed(fail_if_warning= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Suite' object has no attribute 'clean_conditions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27332\\4049668881.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation_suite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_conditions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mevaluation_suite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_condition_train_test_relative_degradation_less_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mevaluation_suite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_suite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Suite' object has no attribute 'clean_conditions'"
     ]
    }
   ],
   "source": [
    "evaluation_suite[0].clean_conditions()\n",
    "evaluation_suite[0].add_condition_train_test_relative_degradation_less_than(0.3)\n",
    "evaluation_suite = evaluation_suite.remove(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_suite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_55924\\2704740211.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_suite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgbr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpassed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfail_if_warning\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluation_suite' is not defined"
     ]
    }
   ],
   "source": [
    "result = evaluation_suite.run(train_ds, test_ds, gbr)\n",
    "result.passed(fail_if_warning= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecdb0ab02a620289a6dfbe50a736c56e802153a4c20e0b9c004cb86daac40531"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
